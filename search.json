[{"title":"如何在Python中的子进程获取键盘输入","url":"/2022/11/25/10-58-00/","content":"<p>场景：在Python中使用multiprocessing模块的Process创建子进程，试图在子进程中获取键盘输入。</p>\n<h1 id=\"使用input\"><a href=\"#使用input\" class=\"headerlink\" title=\"使用input()\"></a>使用input()</h1><p>在子进程中使用input（）会弹出报错信息：EOFError: EOF when reading a line。</p>\n<h2 id=\"代码示例\"><a href=\"#代码示例\" class=\"headerlink\" title=\"代码示例\"></a>代码示例</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Process</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_input</span>():</span><br><span class=\"line\">    info = <span class=\"built_in\">input</span>()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start print info!&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(info)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start progress!&quot;</span>)</span><br><span class=\"line\">    Process(target=test_input).start()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"结果输出\"><a href=\"#结果输出\" class=\"headerlink\" title=\"结果输出\"></a>结果输出</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">start progress!</span><br><span class=\"line\">Process Process-1:</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;D:\\software\\Python\\lib\\multiprocessing\\process.py&quot;, line 258, in _bootstrap</span><br><span class=\"line\">    self.run()</span><br><span class=\"line\">  File &quot;D:\\software\\Python\\lib\\multiprocessing\\process.py&quot;, line 93, in run</span><br><span class=\"line\">    self._target(*self._args, **self._kwargs)</span><br><span class=\"line\">  File &quot;D:\\text_project\\python\\验收2\\test.py&quot;, line 5, in test_input</span><br><span class=\"line\">    info = input()</span><br><span class=\"line\">EOFError: EOF when reading a line</span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code 0</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"使用sys-stdin-readline\"><a href=\"#使用sys-stdin-readline\" class=\"headerlink\" title=\"使用sys.stdin.readline()\"></a>使用sys.stdin.readline()</h1><p>在子进程中使用sys.stdin.readline()，发现并不会等待键盘输入。</p>\n<h2 id=\"代码示例-1\"><a href=\"#代码示例-1\" class=\"headerlink\" title=\"代码示例\"></a>代码示例</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Process</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_input</span>():</span><br><span class=\"line\">    info = sys.stdin.readline()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start print info!&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(info)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start progress!&quot;</span>)</span><br><span class=\"line\">    Process(target=test_input).start()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"结果输出-1\"><a href=\"#结果输出-1\" class=\"headerlink\" title=\"结果输出\"></a>结果输出</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">start progress!</span><br><span class=\"line\">start print info!</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code 0</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"使用fn-x3D-sys-stdin-fileno\"><a href=\"#使用fn-x3D-sys-stdin-fileno\" class=\"headerlink\" title=\"使用fn&#x3D;sys.stdin.fileno()\"></a>使用fn&#x3D;sys.stdin.fileno()</h1><p>在主进程中敲写代码fn&#x3D;sys.stdin.fileno()，然后将获取到的文件描述符fn传入子进程，子进程敲写代码sys.stdin &#x3D; os.fdopen(fn)，然后就可以正常使用sys.stdin.readline()获取键盘输入了。</p>\n<h2 id=\"代码示例-2\"><a href=\"#代码示例-2\" class=\"headerlink\" title=\"代码示例\"></a>代码示例</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Process</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test_input</span>(<span class=\"params\">fn</span>):</span><br><span class=\"line\">    sys.stdin = os.fdopen(fn)</span><br><span class=\"line\">    info = sys.stdin.readline()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start print info!&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(info)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;start progress!&quot;</span>)</span><br><span class=\"line\">    fn = sys.stdin.fileno()</span><br><span class=\"line\">    Process(target=test_input, args=(fn, )).start()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"结果输出-2\"><a href=\"#结果输出-2\" class=\"headerlink\" title=\"结果输出\"></a>结果输出</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">start progress!</span><br><span class=\"line\">this is my input.</span><br><span class=\"line\">start print info!</span><br><span class=\"line\">this is my input.</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Process finished with exit code 0</span><br></pre></td></tr></table></figure>","categories":["疑难杂症"]},{"title":"RNN的PyTorch实现","url":"/2022/11/18/22-17-53/","content":"<h1 id=\"官方实现\"><a href=\"#官方实现\" class=\"headerlink\" title=\"官方实现\"></a>官方实现</h1><p>PyTorch已经实现了一个RNN类，就在torch.nn工具包中，通过torch.nn.RNN调用。</p>\n<p>使用步骤：</p>\n<ol>\n<li>实例化类；</li>\n<li>将输入层向量和隐藏层向量初始状态值传给实例化后的对象，获得RNN的输出。</li>\n</ol>\n<p>在实例化该类时，需要传入如下属性：</p>\n<ul>\n<li>input_size：输入层神经元个数；</li>\n<li>hidden_size：每层隐藏层的神经元个数；</li>\n<li>num_layers：隐藏层层数，默认设置为1层；</li>\n<li>nonlinearity：激活函数的选择，可选是’tanh’或者’relu’，默认设置为’tanh’；</li>\n<li>bias：偏置系数，可选是’True’或者’False’，默认设置为’True’；</li>\n<li>batch_first：可选是’True’或者’False’，默认设置为’False’；</li>\n<li>dropout：默认设置为0。若为非0，将在除最后一层的每层RNN输出上引入Dropout层，dropout概率就是该非零值；</li>\n<li>bidirectional：默认设置为False。若为True，即为双向RNN。</li>\n</ul>\n<p>RNN的输入有两个，一个是input，一个是h<sub>0</sub>。input就是输入层向量，h<sub>0</sub>就是隐藏层初始状态值。<br>若没有采用批量输入，则输入层向量的形状为(L, H<sub>in</sub>)；<br>若采用批量输入，且batch_first为False，则输入层向量的形状为(L, N, H<sub>in</sub>)；<br>若采用批量输入，且batch_first为True，则输入层向量的形状为(N, L, H<sub>in</sub>)；<br>对于(N, L, H<sub>in</sub>)，在文本输入时，可以按顺序理解为(每次输入几句话，每句话有几个字，每个字由多少维的向量表示)。</p>\n<p>若没有采用批量输入，则隐藏层向量的形状为(D * num_layers, H<sub>out</sub>)；<br>若采用批量输入，则隐藏层向量的形状为(D * num_layers, N, H<sub>out</sub>)；<br>注意，batch_first的设置对隐藏层向量的形状不起作用。</p>\n<p>RNN的输出有两个，一个是output，一个是h<sub>n</sub>。output包含了每个时间步最后一层的隐藏层状态，h<sub>n</sub>包含了最后一个时间步每层的隐藏层状态。<br>若没有采用批量输入，则输出层向量的形状为(L, D * H<sub>out</sub>)；<br>若采用批量输入，且batch_first为False，则输出层向量的形状为(L, N, D * H<sub>out</sub>)；<br>若采用批量输入，且batch_first为True，则输出层向量的形状为(N, L, D * H<sub>out</sub>)。</p>\n<p>参数解释：</p>\n<ul>\n<li>N代表的是批量大小；</li>\n<li>L代表的是输入的序列长度；</li>\n<li>若是双向RNN，则D的值为2；若是单向RNN，则D的值为1；</li>\n<li>H<sub>in</sub>在数值上是输入层神经元个数；</li>\n<li>H<sub>out</sub>在数值上是隐藏层神经元个数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\">rnn = nn.RNN(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">1</span>, batch_first=<span class=\"literal\">True</span>)  <span class=\"comment\"># 实例化一个单向单层RNN</span></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">h0 = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">output, hn = rnn(<span class=\"built_in\">input</span>, h0)</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"手写复现\"><a href=\"#手写复现\" class=\"headerlink\" title=\"手写复现\"></a>手写复现</h1><h2 id=\"复现代码\"><a href=\"#复现代码\" class=\"headerlink\" title=\"复现代码\"></a>复现代码</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyRNN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.input_size = input_size</span><br><span class=\"line\">        self.hidden_size = hidden_size</span><br><span class=\"line\">        self.weight_ih = torch.randn(self.hidden_size, self.input_size) * <span class=\"number\">0.01</span></span><br><span class=\"line\">        self.weight_hh = torch.randn(self.hidden_size, self.hidden_size) * <span class=\"number\">0.01</span></span><br><span class=\"line\">        self.bias_ih = torch.randn(self.hidden_size)</span><br><span class=\"line\">        self.bias_hh = torch.randn(self.hidden_size)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span>, h0</span>):</span><br><span class=\"line\">        N, L, input_size = <span class=\"built_in\">input</span>.shape</span><br><span class=\"line\">        output = torch.zeros(N, L, self.hidden_size)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(L):</span><br><span class=\"line\">            x = <span class=\"built_in\">input</span>[:, t, :].unsqueeze(<span class=\"number\">2</span>)  <span class=\"comment\"># 获得当前时刻的输入特征，[N, input_size, 1]。unsqueeze(n)，在第n维上增加一维</span></span><br><span class=\"line\">            w_ih_batch = self.weight_ih.unsqueeze(<span class=\"number\">0</span>).tile(N, <span class=\"number\">1</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size, input_size]</span></span><br><span class=\"line\">            w_hh_batch = self.weight_hh.unsqueeze(<span class=\"number\">0</span>).tile(N, <span class=\"number\">1</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size, hidden_size]</span></span><br><span class=\"line\">            w_times_x = torch.bmm(w_ih_batch, x).squeeze(-<span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size]。squeeze(n)，在第n维上减小一维</span></span><br><span class=\"line\">            w_times_h = torch.bmm(w_hh_batch, h0.unsqueeze(<span class=\"number\">2</span>)).squeeze(-<span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size]</span></span><br><span class=\"line\">            h0 = torch.tanh(w_times_x + self.bias_ih  + w_times_h + self.bias_hh)</span><br><span class=\"line\">            output[:, t, :] = h0</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, h0.unsqueeze(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"验证正确性\"><a href=\"#验证正确性\" class=\"headerlink\" title=\"验证正确性\"></a>验证正确性</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">my_rnn = MyRNN(<span class=\"number\">10</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">h0 = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">my_output, my_hn = my_rnn(<span class=\"built_in\">input</span>, h0)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape == my_output.shape, hn.shape == my_hn.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>True True\n</code></pre>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\">官方说明文档</a></p>\n","categories":["NLP"]},{"title":"《Hierarchical Text-Conditional Image Generation with CLIP Latents》阅读笔记","url":"/2022/11/19/22-05-53/","content":"<h1 id=\"概括\"><a href=\"#概括\" class=\"headerlink\" title=\"概括\"></a>概括</h1><h2 id=\"模型总述\"><a href=\"#模型总述\" class=\"headerlink\" title=\"模型总述\"></a>模型总述</h2><p>本篇论文主要介绍DALL·E 2模型，它是OpenAI在2022年4月推出的一款模型，OpenAI在2021年1月推出了DALL·E模型，2021年年底推出了GLIDE模型。</p>\n<p>DALL·E 2可以根据文本描述去生成原创性的、真实的图像，这些图像从来没有在训练集里出现过，模型真的学习到了文本图像特征，可以任意地组合其概念、属性、风格。</p>\n<p>DALL·E 2除了根据文本生成图像，还能根据文本对已有的图像进行编辑和修改——可以任意添加或者移除图像里的物体，修改时甚至可以把阴影、光线和物体纹理都考虑在内。</p>\n<p>DALL·E 2可以在没有文本输入的情况下，做一些图像生成的工作——比如给定一张图像，它可以根据已有的图像和它的风格，去生成很多类似这种风格的图像。</p>\n<p>DALL·E 2颠覆了人们对于AI的传统理解——AI不止可以处理重复性工作，也能胜任创造性工作。</p>\n<p>DALL·E 2和DALL·E相比，分辨率是前者的四倍，且生成的图像更真实，与文本描述更贴切。</p>\n<p>考虑到安全性和伦理性方面，DALL·E 2没有开源，连API也没有开放。</p>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><p><img src=\"/2022/11/19/22-05-53/%E5%8E%86%E7%A8%8B.png\" alt=\"历程\"></p>\n<h1 id=\"标题\"><a href=\"#标题\" class=\"headerlink\" title=\"标题\"></a>标题</h1><p>基于CLIP的分层文本条件图像生成——使用CLIP训练好的特征，来做层级式的依托于文本的图像生成工作。</p>\n<p>所谓的层级式，意思是DALL·E 2模型是先生成一个64 * 64的小分辨率图像，再利用一个模型上采样到256 * 256，然后继续利用一个模型上采样到1024 * 1024，得到最终的一个高清大图，所以说是一个层级式的结构。</p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>本文提出了一个两阶段的模型：</p>\n<ul>\n<li>prior根据给定的文本描述，生成类似于CLIP的图像特征，prior尝试了自回归模型和扩散模型，发现扩散模型效率高且效果好；</li>\n<li>decoder根据生成的图像特征生成图像，decoder用的是扩散模型。</li>\n</ul>\n<p>该模型有两个亮点：</p>\n<ul>\n<li>生成的图像的写实程度和文本匹配度非常高；</li>\n<li>可以实时利用文本信息引导模型生成、编辑各种图像，且不需要训练（zero-shot）。</li>\n</ul>\n<p><img src=\"/2022/11/19/22-05-53/%E6%91%98%E8%A6%81.png\" alt=\"摘要\"></p>\n<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p>CLIP的优点：</p>\n<ul>\n<li>对图像分布的变化很敏感，具有很强的zero-shot能力，并经过微调，在各式的视觉和语言任务中表现优异。</li>\n</ul>\n<p>生成扩散模型的优点：</p>\n<ul>\n<li>很有前景，在近期的研究中，它利用了一种引导技术，通过牺牲一部分多样性，从而达到更好的图像保真度。</li>\n</ul>\n<p><img src=\"/2022/11/19/22-05-53/%E5%BC%95%E8%A8%80.png\" alt=\"引言\"></p>\n<p>在图示中，虚线的上半部分是CLIP的训练过程，虚线的下半部分描述的DALL·E 2的训练过程。</p>\n<p>对于CLIP，在训练时，将文本以及对应的图像分别输入文本编码器和图像编码器，然后得到输出的文本特征和图像特征，这两个特征就是一个正样本，该文本特征与其他图像生成的图像特征就是负样本，通过对比学习，训练文本编码器和图像编码器，从而实现文本的特征和图像的特征联系在一起，成为一个合并的多模态的特征空间。一旦CLIP模型训练结束，文本编码器和图像编码器就冻结了。在DALL·E 2的训练过程中，CLIP就处于冻结状态，没有进行任何训练和微调。</p>\n<p>对于DALL·E 2，在训练时，首先将文本和对应的图像分别输入CLIP的文本编码器和图像编码器，在拿到文本特征后，将其喂入prior模型，由它生成图像特征，在这个过程中，由CLIP图像编码器生成的图像特征充当了ground truth的角色进行监督；在推理时，也就是只有文本没有配对图像的时候，其过程就是将文本输入CLIP文本编码器生成文本特征，文本特征通过prior模型生成图像特征，图像特征通过扩散模型生成最后的图像。DALL·E 2其实就是把CLIP和GLIDE合在一起，GLIDE模型是一个基于扩散模型的文本图像生成的方法。</p>\n<p>DALL·E 2也被作者称为unCLIP。对于CLIP来说，它是给定文本和图像，然后得到特征，可以拿特征去做图像匹配、图像检索之类的工作，是一个从输入到特征的过程；对于DALL·E 2来说，它是通过文本特征，然后到图像特征，最后到图像的过程，其实就是CLIP的反过程，把特征又还原到数据，所以整个框架叫做unCLIP。</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><p>训练数据集采用图像文本对，给定图像x，用z<sub>i</sub>表示从CLIP出来的图像特征，z<sub>t</sub>表示从CLIP出来的文本特征，整个模型的网络结构被分成prior模型和decoder模型。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E6%96%B9%E6%B3%95.png\" alt=\"方法\"></p>\n<p>首先准备一个CLIP模型，然后训练DALL·E 2的图像生成模型——给定任意文本将它通过CLIP的文本编码器，得到一个文本特征，然后用prior模型把文本特征变成图像特征，再通过一个解码器，把图像特征变成了几张图像。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%85%AC%E5%BC%8F.png\" alt=\"公式\"></p>\n<p>P(x|y)表示给定一个文本y，生成图像x的概率；</p>\n<p>P(x,z<sub>i</sub>|y)表示给定一个文本y，生成图像x和图像特征z<sub>i</sub>的概率。因为图像特征z<sub>i</sub>和图像是一对一的关系，所以z<sub>i</sub>和x是对等的，所以左边第一个等号成立；</p>\n<p>P(x|z<sub>i</sub>,y)表示给定一个文本y和图像特征z<sub>i</sub>去生成图像x的概率（decoder）；</p>\n<p>P(z<sub>i</sub>|y)表示给定一个文本y，生成图像特征z<sub>i</sub>的概率（prior）。</p>\n<h2 id=\"decoder\"><a href=\"#decoder\" class=\"headerlink\" title=\"decoder\"></a>decoder</h2><p>本文的解码器其实就是GLIDE模型的变体，用了CLIP guidance和classifier-free guidance。</p>\n<p>guidance信号要么来自CLIP模型，要么来自于文本，作者随机设10%的时间令CLIP的特征为0，并且训练的时候有50%的时间把文本直接丢弃。</p>\n<p>在生成图像时采用级联式生成的方法，由64 * 64逐步生成得到1024 * 1024的高清大图，为了训练的稳定性，在训练时加了很多噪声。</p>\n<h2 id=\"prior\"><a href=\"#prior\" class=\"headerlink\" title=\"prior\"></a>prior</h2><p><img src=\"/2022/11/19/22-05-53/prior.png\" alt=\"prior\"></p>\n<p>prior模型不论是用自回归模型还是扩散模型，都使用了classifier-free guidance。</p>\n<p>对于扩散prior来说，作者训练了一个Transformer的decoder，因为它的输入输出是embedding，所以不合适用U-Net，选择直接用Transformer处理这个序列。</p>\n<h1 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h1><h2 id=\"图像生成\"><a href=\"#图像生成\" class=\"headerlink\" title=\"图像生成\"></a>图像生成</h2><p>生成给定图像的很多类似图像，所生成的图像风格和原始图像一致，图像中所出现的物体也大体一致。</p>\n<p>其方法是当用户给定一张图像的时候，通过CLIP的图像编码器得到一个图像特征，把图像特征变成文本特征，再把文本特征输入给prior模型生成另外一个图像特征，这个图像特征再生成新的图像。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%BA%94%E7%94%A81.png\" alt=\"应用1\"></p>\n<h2 id=\"两个图像之间做内插\"><a href=\"#两个图像之间做内插\" class=\"headerlink\" title=\"两个图像之间做内插\"></a>两个图像之间做内插</h2><p>给定两张图像，在两张图像的图像特征之间做内插，当插出来的特征更偏向于某个图像时，所生成的图像也就更多地具有该图像的特征。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%BA%94%E7%94%A82.png\" alt=\"应用2\"></p>\n<h2 id=\"两个文本之间做内插\"><a href=\"#两个文本之间做内插\" class=\"headerlink\" title=\"两个文本之间做内插\"></a>两个文本之间做内插</h2><p><img src=\"/2022/11/19/22-05-53/%E5%BA%94%E7%94%A83.png\" alt=\"应用3\"></p>\n<h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><p><img src=\"/2022/11/19/22-05-53/result.png\" alt=\"result\"></p>\n<h1 id=\"不足\"><a href=\"#不足\" class=\"headerlink\" title=\"不足\"></a>不足</h1><p>不能很好地把物体和属性联系起来（很有可能是CLIP模型的原因）。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%B1%80%E9%99%901.png\" alt=\"局限1\"></p>\n<p>当生成的图像里有文字时，文字是错误的（有可能是文本编码器使用了BPE编码）。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%B1%80%E9%99%902.png\" alt=\"局限2\"></p>\n<p>不能生成特别复杂的场景，很多细节生成不出来。</p>\n<p><img src=\"/2022/11/19/22-05-53/%E5%B1%80%E9%99%903.png\" alt=\"局限3\"></p>\n<h1 id=\"相关概念\"><a href=\"#相关概念\" class=\"headerlink\" title=\"相关概念\"></a>相关概念</h1><h2 id=\"Generative-Adversarial-Networks（GAN）：生成对抗网络\"><a href=\"#Generative-Adversarial-Networks（GAN）：生成对抗网络\" class=\"headerlink\" title=\"Generative Adversarial Networks（GAN）：生成对抗网络\"></a>Generative Adversarial Networks（GAN）：生成对抗网络</h2><p>GAN包含有两个模型，一个是生成模型（generative model），一个是判别模型(discriminative model)。</p>\n<p>生成模型的任务是生成看起来自然真实的、和原始数据相似的实例。判别模型的任务是判断给定的实例看起来是自然真实的还是人为伪造的（真实实例来源于数据集，伪造实例来源于生成模型）。</p>\n<p>这可以看做一种零和游戏，生成模型像“一个造假团伙，试图生产和使用假币”，而判别模型像“检测假币的警察”。生成器（generator）试图欺骗判别器（discriminator），判别器则努力不被生成器欺骗。模型经过交替优化训练，两种模型都能得到提升，但最终我们要得到的是效果提升到很高很好的生成模型（造假团伙），这个生成模型（造假团伙）所生成的产品能达到真假难分的地步。因为GAN的目标函数是用来以假乱真的，所以截至目前为止，GAN生成的图像保真度非常高，引燃了DeepFakes的火爆，但是它的多样性不好，且不太具备原创性，这也是最近的模型如DALL·E 2和Imagen都是用扩散模型的原因，因为扩散模型多样性好，还有创造力。</p>\n<p>GAN的缺点</p>\n<ul>\n<li>训练不够稳定，最主要的原因是要同时训练两个网络，所以有一个平衡的问题，容易发生模型的训练坍塌；</li>\n<li>生成图像的多样性不好，本质是创造性不好；</li>\n<li>不是一个概率模型，它的生成都是隐式的，就是通过一个网络去完成，无法获知它到底做了什么、遵循了什么分布，所以GAN在数学上不如后续的VAE，或者扩散模型。</li>\n</ul>\n<h2 id=\"Auto-Encoder（AE）：自编码器\"><a href=\"#Auto-Encoder（AE）：自编码器\" class=\"headerlink\" title=\"Auto-Encoder（AE）：自编码器\"></a>Auto-Encoder（AE）：自编码器</h2><p>是一种无监督式的学习模型，它基于反向传播算法与最优化方法（如梯度下降法），利用输入数据X本身作为监督，来指导神经网络尝试学习一个映射关系，从而得到一个重构输出X<sup>R</sup>。</p>\n<p>算法模型包含Encoder（编码器）和Decoder（解码器）。</p>\n<p>编码器的作用是把高维输入X编码成低维隐变量h从而强迫神经网络学习最有信息量的特征；</p>\n<p>解码器的作用是把隐藏层的隐变量h还原到初始维度，最好的状态就是解码器的输出能够完美地或者近似恢复出原来的输入， 即X<sup>R</sup>≈X  。</p>\n<p><img src=\"/2022/11/19/22-05-53/AE4.png\" alt=\"AE4\"></p>\n<p>从输入层 -&gt;隐藏层的原始数据X的编码过程：</p>\n<p><img src=\"/2022/11/19/22-05-53/AE1.png\" alt=\"AE1\"></p>\n<p>从隐藏层 -&gt; 输出层的解码过程：</p>\n<p><img src=\"/2022/11/19/22-05-53/AE2.png\" alt=\"AE2\"></p>\n<p>算法的优化目标函数： </p>\n<p><img src=\"/2022/11/19/22-05-53/AE3.png\" alt=\"AE3\"></p>\n<p>其中dist为二者的距离度量函数，通常用MSE（均方方差）。</p>\n<p>自编码可以实现类似于PCA等数据降维、数据压缩的特性。如果输入层神经元的个数n大于隐层神经元个数m，那么就相当于把数据从n维降到了m维；然后可以利用这m维的特征向量，重构原始的数据。这个跟PCA降维一模一样，只不过PCA是通过求解特征向量进行降维，是一种线性的降维方式，而自编码是一种非线性降维。</p>\n<h2 id=\"Denoising-Auto-Encoder（DAE）：去噪自编码器\"><a href=\"#Denoising-Auto-Encoder（DAE）：去噪自编码器\" class=\"headerlink\" title=\"Denoising Auto-Encoder（DAE）：去噪自编码器\"></a>Denoising Auto-Encoder（DAE）：去噪自编码器</h2><p>先向输入注入噪声，然后把经过扰乱的输入传给编码器，让解码器重构不含噪声的输入。</p>\n<p>这种方法可以让训练的模型非常稳健，不容易过拟合，部分原因是因为图像像素冗余度太高了，所以即使把原来的图像做一些污染，模型还是能抓住它的本质，将它重建出来。</p>\n<h2 id=\"Variational-Auto-Encoder（VAE）：变分自编码器\"><a href=\"#Variational-Auto-Encoder（VAE）：变分自编码器\" class=\"headerlink\" title=\"Variational  Auto-Encoder（VAE）：变分自编码器\"></a>Variational  Auto-Encoder（VAE）：变分自编码器</h2><p>VAE其实跟AE很不一样，它不再是学习固定的隐藏层特征了，而是在学习一种分布，比如假设这个分布是一个高斯分布，可以用均值和方差描述，编码器不再直接输出h，而是输出h分布的均值和方差，再从这个分布中采样得到h，然后h再通过解码器。简而言之，VAE预测的是一个分布。</p>\n<h2 id=\"Vector-Quantized-VAE（VQ-VAE）：向量量化的变分自编码器\"><a href=\"#Vector-Quantized-VAE（VQ-VAE）：向量量化的变分自编码器\" class=\"headerlink\" title=\"Vector Quantized VAE（VQ-VAE）：向量量化的变分自编码器\"></a>Vector Quantized VAE（VQ-VAE）：向量量化的变分自编码器</h2><p>就是把VAE做量化，它采用的是离散的隐变量，不像VAE那样采用连续的隐变量。</p>\n<h2 id=\"VQ-VAE2\"><a href=\"#VQ-VAE2\" class=\"headerlink\" title=\"VQ-VAE2\"></a>VQ-VAE2</h2><p>把隐空间分成了两个，一个上层隐空间(top latent space)，一个下层隐空间(bottom latent space)。上层隐向量 用于表示全局信息，下层隐向量 用于表示 局部信息。</p>\n<h2 id=\"DALL·E\"><a href=\"#DALL·E\" class=\"headerlink\" title=\"DALL·E\"></a>DALL·E</h2><p>由OpenAI提出的能根据文本描述生成类似超现实主义图像的图像生成器。</p>\n<h2 id=\"autoregressive-models（AR）：自回归模型\"><a href=\"#autoregressive-models（AR）：自回归模型\" class=\"headerlink\" title=\"autoregressive models（AR）：自回归模型\"></a>autoregressive models（AR）：自回归模型</h2><p>自回归模型是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即x<sub>1</sub>至x<sub>t-1</sub>来预测本期x<sub>t</sub>的表现，并假设它们为线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测 x（自己）,所以叫做自回归。神经网络中的自回归模型，将联合概率拆成了条件概率累乘的形式。</p>\n<h2 id=\"diffusion-models：生成扩散模型\"><a href=\"#diffusion-models：生成扩散模型\" class=\"headerlink\" title=\"diffusion models：生成扩散模型\"></a>diffusion models：生成扩散模型</h2><p>diffusion models名字来源于热力学的启发，工作原理从本质上来说是通过连续添加高斯噪声来破坏训练数据，然后通过反转这个噪声过程，来学习恢复数据。</p>\n<p>它是Encoder-Decoder架构的模型，分为扩散阶段和逆扩散阶段。 </p>\n<ul>\n<li><p>在扩散阶段，通过不断对原始数据添加噪声，使数据从原始分布变为我们期望的分布，例如通过不断添加高斯噪声将原始数据分布变为正态分布。 </p>\n</li>\n<li><p>在逆扩散阶段，使用神经网络（U-Net，一个CNN）将数据从正态分布恢复到原始数据分布。</p>\n</li>\n</ul>\n<p>训练后，可以使用该模型将原始输入的图像去噪生成新图像。</p>\n<p>优点是正态分布上的每个点都是真实数据的映射，模型具有更好的可解释性。缺点是迭代采样速度慢，导致模型训练和预测效率低。</p>\n<p>扩散模型早在2015年或者更早的时候就被提出来了，但当时只是一个想法，一直到2020年6月DDPM提出来后，扩散模型才开始火爆，DDPM算是扩散模型的开山之作。</p>\n<p>DDPM对原始的扩散模型做了一些改进，把优化过程变得简单，有两个最重要的贡献：</p>\n<ul>\n<li>之前大家都是x<sub>t</sub>预测x<sub>t-1</sub>，做图像到图像的转化，而DDPM预测x<sub>t-1</sub>到x<sub>t</sub>的噪声是怎么加的，有点像ResNet，本来是直接用x预测y，现在理解成y&#x3D;x+residual，转而预测残差residual就可以了；</li>\n<li>原本预测一个正态分布，要去学它的均值和方差，作者提出可以把方差视为一个常数，只要去预测均值就可以了，再次降低了模型优化的难度。</li>\n</ul>\n<p>DDPM和VAE有很多相似之处，都是编码器、解码器的结构，不同点在于：</p>\n<ul>\n<li>在扩散模型中，编码器的一步步运算是一个固定的过程，对于VAE来说，编码器则不是这样；</li>\n<li>在扩散模型中，每一步中间过程的输出跟刚开始的输入都是同样维度的大小，对于VAE来说，它中间的bottleneck特征往往比输入小得多；</li>\n<li>在扩散模型中，从随机噪声开始，要经过很多步才能生成一个图像，所以它有time step、time embedding的概念，而且在所有的time step里面它的U-Net模型都是共享参数的，在VAE里就不存在这一点。</li>\n</ul>\n<p>improved DDPM相较于DDPM做了一些改进：</p>\n<ul>\n<li>学习了正态分布里的方差，效果更好；</li>\n<li>把添加噪声的schedule改了，从一个线性的schedule改成了余弦的schedule，效果更好；</li>\n<li>简单尝试了一下给扩散模型上更大的模型，效果更好。</li>\n</ul>\n<p>基于improved DDPM的第三个改进，有人发表了论文《Diffusion model beats GAN》，即扩散模型比GAN强。在文中：</p>\n<ul>\n<li>作者把模型加大加宽，增加自注意力头（attention head）的数量；</li>\n<li>发现single-scale的attention不够用，改用multi-scale的attention；</li>\n<li>提出新的归一化方式，叫做adaptive group normalization，根据步数去做自适应的归一化；</li>\n<li>使用classifier guidance的方法，去引导模型做采样和生成，这不仅让生成的图像更加逼真，而且也加速了方向采样的速度。论文中表明做25次采样，就能从一个噪声生成一个非常好的图像。</li>\n</ul>\n<p><img src=\"/2022/11/19/22-05-53/classifier-guidance.png\" alt=\"classifier-guidance\"></p>\n<p>classifier guidance diffusion是说在我们训练扩散模型的同时，再训练一个图像分类器，这个分类器是在ImagNet上的图像加上噪声训练来的。分类器的作用是对于图像x<sub>t</sub>，它可以计算一个交叉熵目标函数，得到一些梯度，然后使用这个梯度，去帮助模型进行采样和图像生成。分类器可以根据需求进行选择，把分类器换成CLIP模型，那么文本和图像就联系起来了，此时我们不光可以利用梯度去引导模型的采样和生成，甚至可以利用文本去控制图像的采样和生成。</p>\n<p>classifier guidance diffusion的成本很高，它要求我们要么有一个pre-trained的模型，要么得重新训练一个模型，所以引出来后续的classifier-free guidance。</p>\n<p>classifier-free guidance不使用分类器，而是在训练模型的时候让它生成两个输出，一个是在有条件时生成的，一个是在无条件时生成的。比如用图像文本对训练的时候，用文本去做这个guidance信号，生成一个图像，然后不用这个文本，而用一个空的序列，再去生成另外一个图像。假设生成的两个图像在一个空间里，那么就会有一个方向能从这种无文本得到的图像指向有文本得到的图像，通过训练得到二者之间的距离，那么等到反向扩散的时候，即使我们的图像输出是没有使用文本生成的，我们也能做出一个比较合理的推理，从一个没有条件生成的x变成一个有条件生成的x，摆脱分类器的限制。这个方法因为产生两个输出，模型的训练成本是很大的。在GLIDE、DALL·E 2、Imagen都使用了classifier-free guidance。</p>\n<h2 id=\"Contrastive-Learning：对比学习\"><a href=\"#Contrastive-Learning：对比学习\" class=\"headerlink\" title=\"Contrastive Learning：对比学习\"></a>Contrastive Learning：对比学习</h2><p>是一种自监督学习方法，用于在没有标签的情况下，通过让模型学习哪些数据点相似或不同来学习数据集的一般特征。</p>\n<h2 id=\"zero-shot-learning：零次学习\"><a href=\"#zero-shot-learning：零次学习\" class=\"headerlink\" title=\"zero-shot learning：零次学习\"></a>zero-shot learning：零次学习</h2><p>它是利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集，期间是借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。对于要分类的类别对象，是一次也不学习的。</p>\n<h2 id=\"PCA（-Principal-Component-Analysis-）：主成分分析\"><a href=\"#PCA（-Principal-Component-Analysis-）：主成分分析\" class=\"headerlink\" title=\"PCA（ Principal Component Analysis ）：主成分分析\"></a>PCA（ Principal Component Analysis ）：主成分分析</h2><p>是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。</p>\n<h2 id=\"Embedding：嵌入层\"><a href=\"#Embedding：嵌入层\" class=\"headerlink\" title=\"Embedding：嵌入层\"></a>Embedding：嵌入层</h2><p>它能把万物嵌入万物，是沟通两个世界的桥梁。数学定义即：它是单射且同构的。</p>\n<p>简单来说，我们常见的地图就是对于现实地理的Embedding，现实的地理地形的信息其实远远超过三维，但是地图通过颜色和等高线等来最大化表现现实的地理信息。通过它，我们在现实世界里的文字、图像、语言、视频就能转化为计算机能识别、能使用的语言，且转化的过程中信息不丢失。</p>\n<p>可以通过矩阵乘法进行降维，假如我们有一个100W X10W的矩阵，用它乘上一个10W X 20的矩阵，我们可以把它降到100W X 20，瞬间量级降了10W&#x2F;20&#x3D;5000倍；也可以进行升维，对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了。</p>\n<h3 id=\"Feature-Embedding：特征嵌入\"><a href=\"#Feature-Embedding：特征嵌入\" class=\"headerlink\" title=\"Feature Embedding：特征嵌入\"></a>Feature Embedding：特征嵌入</h3><p>将数据转换（降维）为固定大小的特征表示（矢量），以便于处理和计算（如求距离）。</p>\n<h3 id=\"Image-Embedding：图像嵌入\"><a href=\"#Image-Embedding：图像嵌入\" class=\"headerlink\" title=\"Image Embedding：图像嵌入\"></a>Image Embedding：图像嵌入</h3><p>将图像转换成n维的特征向量。</p>\n<h3 id=\"Word-Embedding：词嵌入\"><a href=\"#Word-Embedding：词嵌入\" class=\"headerlink\" title=\"Word Embedding：词嵌入\"></a>Word Embedding：词嵌入</h3><p>将非结构化的文本转换为n维结构化的特征向量。</p>\n<ul>\n<li>可以将文本通过一个低维向量来表达，不像 one-hot 那么长；</li>\n<li>语意相似的词在向量空间上也会比较相近；</li>\n<li>通用性很强，可以用在不同的任务中。</li>\n</ul>\n<h2 id=\"latent-space：潜在空间\"><a href=\"#latent-space：潜在空间\" class=\"headerlink\" title=\"latent space：潜在空间\"></a>latent space：潜在空间</h2><p>原始数据压缩（编码）后的表示（即特征向量）所在的空间，即为潜在空间。</p>\n<ul>\n<li>潜在空间只是压缩数据的表示，其中相似的数据点在空间上更靠近在一起；</li>\n<li>潜在空间对于学习数据特征和查找更简单的数据表示形式以进行分析很有用；</li>\n<li>可以通过分析潜在空间中的数据（通过流形，聚类等）来了解数据点之间的模式或结构相似性；</li>\n<li>可以在潜在空间内插值数据，并使用模型的解码器来“生成”数据样本（比如生成新图像）；</li>\n<li>可以使用t-SNE和LLE之类的算法来可视化潜在空间，该算法将潜在空间表示形式转换为2D或3D。</li>\n</ul>\n<h2 id=\"State-Of-The-Art-（SOTA）\"><a href=\"#State-Of-The-Art-（SOTA）\" class=\"headerlink\" title=\"State Of The Art （SOTA）\"></a>State Of The Art （SOTA）</h2><p>表示效果最好的方法。</p>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><ul>\n<li><a href=\"https://www.bilibili.com/video/BV17r4y1u77B/?spm_id_from=333.337.search-card.all.click&vd_source=48b919231ff82c6c96b7d0acfd50d174\">DALL·E 2【论文精读】</a>；</li>\n<li>网上相关参考资料。</li>\n</ul>\n","categories":["论文阅读"]},{"title":"使用Metaweblog上传MD文件到博客园遇到的几个问题","url":"/2022/11/16/19-02-55/","content":"<h1 id=\"can’t-open-file-‘upload-py’-Errno-2-No-such-file-or-directory\"><a href=\"#can’t-open-file-‘upload-py’-Errno-2-No-such-file-or-directory\" class=\"headerlink\" title=\"can’t open file ‘upload.py’: [Errno 2] No such file or directory\"></a>can’t open file ‘upload.py’: [Errno 2] No such file or directory</h1><p><img src=\"/2022/11/16/19-02-55/q1.png\" alt=\"q1\"></p>\n<p>分析：upload.py文件在pycnblog源代码文件夹下，而cmd命令的执行路径是C:\\WINDOWS\\System32，所以提示找不到文件。</p>\n<p>解决方法：</p>\n<p>1、首先，以管理员身份打开cmd窗口。</p>\n<p><img src=\"/2022/11/16/19-02-55/a11.png\" alt=\"a11\"></p>\n<p>2、然后，找到pycnblog源代码所在的路径，并在该路径下重新执行命令。</p>\n<p><img src=\"/2022/11/16/19-02-55/a12.png\" alt=\"a12\"></p>\n<h1 id=\"AttributeError-module-‘asyncio’-has-no-attribute-‘run’\"><a href=\"#AttributeError-module-‘asyncio’-has-no-attribute-‘run’\" class=\"headerlink\" title=\"AttributeError: module ‘asyncio’ has no attribute ‘run’\"></a>AttributeError: module ‘asyncio’ has no attribute ‘run’</h1><p>分析：这是由于Python中asyncio版本不兼容导致的。</p>\n<p>解决方法：（任选一种方法即可）</p>\n<ul>\n<li>法一：将Python版本升级至3.7及以上。</li>\n<li>法二：打开upload.py文件，将报错的run函数注释或者删除，并在下方添加以下语句。</li>\n</ul>\n<p><code>loop = asyncio.get_event_loop()</code></p>\n<p><code> result = loop.run_until_complete()</code></p>\n<p><img src=\"/2022/11/16/19-02-55/a2.png\" alt=\"a2\"></p>\n<h1 id=\"AttributeError-module-‘asyncio’-has-no-attribute-‘create-task’\"><a href=\"#AttributeError-module-‘asyncio’-has-no-attribute-‘create-task’\" class=\"headerlink\" title=\"AttributeError: module ‘asyncio’ has no attribute ‘create_task’\"></a>AttributeError: module ‘asyncio’ has no attribute ‘create_task’</h1><p>分析：这同样是由于Python中asyncio版本不兼容导致的。</p>\n<p>解决方法：（任选一种方法即可）</p>\n<ul>\n<li>将Python版本升级至3.7及以上。</li>\n<li>打开upload.py文件，将报错的create_task函数改成ensure_future函数。</li>\n</ul>\n<p><img src=\"/2022/11/16/19-02-55/a3.png\" alt=\"a3\"></p>\n<h1 id=\"xmlrpc-client-Fault：-lt-Fault401：’请配置正确的用户名与访问令牌-access-token-，密码登录已取消，请在密码框中输入访问令牌…\"><a href=\"#xmlrpc-client-Fault：-lt-Fault401：’请配置正确的用户名与访问令牌-access-token-，密码登录已取消，请在密码框中输入访问令牌…\" class=\"headerlink\" title=\"xmlrpc.client.Fault：&lt;Fault401：’请配置正确的用户名与访问令牌(access token)，密码登录已取消，请在密码框中输入访问令牌…\"></a>xmlrpc.client.Fault：&lt;Fault401：’请配置正确的用户名与访问令牌(access token)，密码登录已取消，请在密码框中输入访问令牌…</h1><p>分析：博客园取消了密码登录，改成了token登录。</p>\n<p>解决方法：根据提示，打开源码文件夹中的config.yaml配置文件，把里面的password字段中原来写的密码值改成博客园的token就行。token在博客园设置页面最底部其他设置那栏里面，如果之前没有生成过token，新生成一个即可。</p>\n<p>1、在首页点击管理；</p>\n<p><img src=\"/2022/11/16/19-02-55/a41.png\" alt=\"a41\"></p>\n<p>2、点击设置；</p>\n<p><img src=\"/2022/11/16/19-02-55/a42.png\" alt=\"a42\"></p>\n<p>3、鼠标拖到页面最下方，找到其他设置，将它点开，MetaWeblog访问令牌就是要找的token。</p>\n<p><img src=\"/2022/11/16/19-02-55/a43.png\" alt=\"a43\"></p>\n<p>4、将config.yaml中的password字段值改成该token值。</p>\n","categories":["疑难杂症"]},{"title":"初学LSTM","url":"/2022/11/24/14-45-24/","content":"","categories":["NLP"]},{"title":"初学RNN","url":"/2022/11/15/21-46-57/","content":"<h1 id=\"FNN\"><a href=\"#FNN\" class=\"headerlink\" title=\"FNN\"></a>FNN</h1><h2 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>FNN（Feedforward Neural Network），即前馈神经网络，它是网络信息单向传递的一种神经网络，数据由输入层开始输入，依次流入隐藏层各层神经元，最终由输出层输出。其当前的输出只由当前的输入决定，任何层的输出都不会影响同级层。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/前馈神经网络-导出.png\" width=\"60%\"> \n</div> \n\n<p>以上图所示的神经网络为例，它在训练过程中通过前向计算和反向传播，不断通过调整权重系数W<sub>i</sub>和W<sub>o</sub>来实现学习目的。通常情况下，前馈神经网络会在空间上进行延伸，通过增加隐藏层层数与隐藏层神经元个数追求更好的学习效果。</p>\n<h2 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h2><p>前馈神经网络假定元素之间是相互独立的，对于序列数据，只能单独地处理序列中的每个元素，前一个输入与后一个输入在处理过程中也是完全独立的，无法捕获序列之间的依赖关系。</p>\n<h1 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h1><h2 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>RNN（Recurrent Neural Network），即循环神经网络，它是一种主要用来处理序列数据的神经网络，它关注了隐藏层每个神经元在时间维度上的变化，其中循环说成递推可能会更直观一些，本质就是同一个网络接收当前时刻的输入和上一时刻隐藏层神经元的输出，沿着时序反复迭代以实现对序列数据的学习。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><h3 id=\"直观结构\"><a href=\"#直观结构\" class=\"headerlink\" title=\"直观结构\"></a>直观结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/RNN-导出.png\" width=\"60%\"> \n</div> \n\n<p>在上图中， RNN 的每个时刻，输入层的x<sub>1</sub>和x<sub>2</sub>都在W<sub>i</sub>的作用下传入隐藏层，上一时刻的隐藏层输出也通过W<sub>h</sub>传入当前的隐藏层，因此它相当于可以间接访问之前的所有输入，这就是为什么说RNN可以保存记忆。</p>\n<h3 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/rnn.svg\" width=\"100%\"> \n</div> \n\n<p>上图展示了RNN在三个相邻时刻的计算逻辑。 在任意时刻t，隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>拼接t时刻的输入X<sub>t</sub>和t−1时刻的隐藏层状态H<sub>t−1</sub>，得到新的张量[X<sub>t</sub>,H<sub>t-1</sub>]；</li>\n<li>将新的张量送入带有激活函数φ的全连接层，激活函数常用tanh或者relu， 全连接层的输出是t时刻的隐藏层状态H<sub>t</sub>。</li>\n</ol>\n<p>在t时刻，隐藏层状态H<sub>t</sub>的计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/H公式.png\" width=\"100%\"> \n</div> \n\n<p>在t时刻，输出层的输出计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/O公式.png\" width=\"100%\"> \n</div> \n\n<p>参数说明：</p>\n<ul>\n<li>X<sub>t</sub>是t时刻的输入，它是一个向量；</li>\n<li>W<sub>i</sub>是输入层到隐藏层的权重矩阵；</li>\n<li>H<sub>t-1</sub>是t-1时刻的隐藏层状态，在初始时刻，会给隐藏层设置初始状态H<sub>0</sub>；</li>\n<li>W<sub>h</sub>是隐藏层上一时刻的值作用于当前时刻的权重矩阵；</li>\n<li>W<sub>o</sub>是隐藏层到输出层的权重矩阵；</li>\n<li>b<sub>h</sub>是和b<sub>o</sub>是偏置系数。</li>\n</ul>\n<p>注意：</p>\n<ul>\n<li>在不同时刻，RNN总是使用这些模型参数，其参数开销不会随着时间的增加而增加。</li>\n<li>隐藏层状态中X<sub>t</sub>W<sub>i</sub>+H<sub>t-1</sub>W<sub>h</sub>的计算，相当于X<sub>t</sub>和H<sub>t-1</sub>的拼接与W<sub>i</sub>和W<sub>h</sub>的拼接进行矩阵乘法。</li>\n<li>RNN和前馈神经网络一样，也是通过反向传播来更新权重，以达到学习的效果。</li>\n</ul>\n<h2 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h2><p>1-N类型，输入一张图片，输出一段话或者一段音乐，利用它可以实现看图说话。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/1-N.png\" width=\"50%\"> \n</div> \n\n<p>N-1类型，输入一段话，输出对其情感类别的判断，利用它可以实现文本分类。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-1.png\" width=\"50%\"> \n</div> \n\n<p>N-N类型，输入和输出是等长的序列，可以用来生成等长的诗歌。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-N.png\" width=\"50%\"> \n</div> \n\n<p>N-M类型，输入和输出是不等长的序列，也被叫做Encoder-Decoder模型或Seq2Seq模型，可以应用在机器翻译、文本摘要、阅读理解等多个领域上。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M.png\" width=\"50%\"> \n</div> \n\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M2.png\" width=\"90%\"> \n</div> \n\n<h2 id=\"变体\"><a href=\"#变体\" class=\"headerlink\" title=\"变体\"></a>变体</h2><h3 id=\"BRNN（Bidirectional-RNN）\"><a href=\"#BRNN（Bidirectional-RNN）\" class=\"headerlink\" title=\"BRNN（Bidirectional RNN）\"></a>BRNN（Bidirectional RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/BRNN.png\" width=\"100%\"> \n</div> \n\n<p>RNN的一个主要问题是只能从以往的输入进行学习，也就是只能理解上下文中的上文，为了拥有更好的学习效果，因此提出了双向RNN，也就是BRNN。 BRNN会在原有RNN的基础上再加一个隐藏层，该隐藏层的状态是从后向前传播的，从序列的终点开始读取，称为后向层；而原有的从序列起点开始读取的隐藏层称为前向层。 BRNN的隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>根据输入序列计算前向层隐藏层状态H<sub>1</sub>；</li>\n<li>将输入序列反转，计算后向层隐藏层状态H<sub>2</sub>；</li>\n<li>将H<sub>1</sub>和H<sub>2</sub>拼接起来，得到最终隐藏层状态H，H&#x3D;[H<sub>1</sub>,H<sub>2</sub>]。</li>\n</ol>\n<p>注意，只有能拿到整个输入序列时才能使用BRNN 。</p>\n<h3 id=\"DRNN（Deep-RNN）\"><a href=\"#DRNN（Deep-RNN）\" class=\"headerlink\" title=\"DRNN（Deep RNN）\"></a>DRNN（Deep RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/DRNN.png\" width=\"50%\"> \n</div> \n\n<p>与前馈神经网络不同，RNN因为考虑了时间维度，隐藏层达到三层就算多的了。</p>\n<h2 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a>优缺点</h2><h3 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h3><ul>\n<li>RNN可以处理序列信息，且内部结构简单，对计算资源的要求低。</li>\n</ul>\n<h3 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><ul>\n<li><p>RNN是一个时序模型，每个时刻的计算都依赖于前一时刻的结果，计算速度慢；</p>\n</li>\n<li><p>RNN由于梯度消失，难以支持长序列，不能捕获序列中长期的依赖关系；</p>\n</li>\n<li><p>RNN网络在时间维度上是串联的，离当前时间越远的隐藏层输出，对当前隐藏层的输出影响越小，它无法根据不同词本身的重要性对当前的输出产生影响；</p>\n</li>\n<li><p>RNN对所有输入是同等对待的，提取了所有的信息，没有区分有用信息、无用信息和辅助信息。但是如果某个网络可以根据不同输入的重要性，选择性地丢弃和记忆，就可以使得有效信息即使距离当前时间较远，也能有较大影响，实现长期记忆，这就引出了LSTM。</p>\n</li>\n</ul>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><ul>\n<li><a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-rnn-w-hidden-states\">《动手学深度学习》第8章第4节</a>；</li>\n<li>网络上相关资料。</li>\n</ul>\n","categories":["NLP"]}]