[{"title":"使用Metaweblog上传MD文件到博客园遇到的几个问题","url":"/2022/11/16/19-02-55/","content":"<h1 id=\"can’t-open-file-‘upload-py’-Errno-2-No-such-file-or-directory\"><a href=\"#can’t-open-file-‘upload-py’-Errno-2-No-such-file-or-directory\" class=\"headerlink\" title=\"can’t open file ‘upload.py’: [Errno 2] No such file or directory\"></a>can’t open file ‘upload.py’: [Errno 2] No such file or directory</h1><p><img src=\"/2022/11/16/19-02-55/q1.png\" alt=\"q1\"></p>\n<p>分析：upload.py文件在pycnblog源代码文件夹下，而cmd命令的执行路径是C:\\WINDOWS\\System32，所以提示找不到文件。</p>\n<p>解决方法：</p>\n<p>1、首先，以管理员身份打开cmd窗口。</p>\n<p><img src=\"/2022/11/16/19-02-55/a11.png\" alt=\"a11\"></p>\n<p>2、然后，找到pycnblog源代码所在的路径，并在该路径下重新执行命令。</p>\n<p><img src=\"/2022/11/16/19-02-55/a12.png\" alt=\"a12\"></p>\n<h1 id=\"AttributeError-module-‘asyncio’-has-no-attribute-‘run’\"><a href=\"#AttributeError-module-‘asyncio’-has-no-attribute-‘run’\" class=\"headerlink\" title=\"AttributeError: module ‘asyncio’ has no attribute ‘run’\"></a>AttributeError: module ‘asyncio’ has no attribute ‘run’</h1><p>分析：这是由于Python中asyncio版本不兼容导致的。</p>\n<p>解决方法：（任选一种方法即可）</p>\n<ul>\n<li>法一：将Python版本升级至3.7及以上。</li>\n<li>法二：打开upload.py文件，将报错的run函数注释或者删除，并在下方添加以下语句。</li>\n</ul>\n<p><code>loop = asyncio.get_event_loop()</code></p>\n<p><code> result = loop.run_until_complete()</code></p>\n<p><img src=\"/2022/11/16/19-02-55/a2.png\" alt=\"a2\"></p>\n<h1 id=\"AttributeError-module-‘asyncio’-has-no-attribute-‘create-task’\"><a href=\"#AttributeError-module-‘asyncio’-has-no-attribute-‘create-task’\" class=\"headerlink\" title=\"AttributeError: module ‘asyncio’ has no attribute ‘create_task’\"></a>AttributeError: module ‘asyncio’ has no attribute ‘create_task’</h1><p>分析：这同样是由于Python中asyncio版本不兼容导致的。</p>\n<p>解决方法：（任选一种方法即可）</p>\n<ul>\n<li>将Python版本升级至3.7及以上。</li>\n<li>打开upload.py文件，将报错的create_task函数改成ensure_future函数。</li>\n</ul>\n<p><img src=\"/2022/11/16/19-02-55/a3.png\" alt=\"a3\"></p>\n<h1 id=\"xmlrpc-client-Fault：-lt-Fault401：’请配置正确的用户名与访问令牌-access-token-，密码登录已取消，请在密码框中输入访问令牌…\"><a href=\"#xmlrpc-client-Fault：-lt-Fault401：’请配置正确的用户名与访问令牌-access-token-，密码登录已取消，请在密码框中输入访问令牌…\" class=\"headerlink\" title=\"xmlrpc.client.Fault：&lt;Fault401：’请配置正确的用户名与访问令牌(access token)，密码登录已取消，请在密码框中输入访问令牌…\"></a>xmlrpc.client.Fault：&lt;Fault401：’请配置正确的用户名与访问令牌(access token)，密码登录已取消，请在密码框中输入访问令牌…</h1><p>分析：博客园取消了密码登录，改成了token登录。</p>\n<p>解决方法：根据提示，打开源码文件夹中的config.yaml配置文件，把里面的password字段中原来写的密码值改成博客园的token就行。token在博客园设置页面最底部其他设置那栏里面，如果之前没有生成过token，新生成一个即可。</p>\n<p>1、在首页点击管理；</p>\n<p><img src=\"/2022/11/16/19-02-55/a41.png\" alt=\"a41\"></p>\n<p>2、点击设置；</p>\n<p><img src=\"/2022/11/16/19-02-55/a42.png\" alt=\"a42\"></p>\n<p>3、鼠标拖到页面最下方，找到其他设置，将它点开，MetaWeblog访问令牌就是要找的token。</p>\n<p><img src=\"/2022/11/16/19-02-55/a43.png\" alt=\"a43\"></p>\n<p>4、将config.yaml中的password字段值改成该token值。</p>\n","categories":["疑难杂症"]},{"title":"RNN的PyTorch实现","url":"/2022/11/18/22-17-53/","content":"<h1 id=\"官方实现\"><a href=\"#官方实现\" class=\"headerlink\" title=\"官方实现\"></a>官方实现</h1><p>PyTorch已经实现了一个RNN类，就在torch.nn工具包中，通过torch.nn.RNN调用。</p>\n<p>使用步骤：</p>\n<ol>\n<li>实例化类；</li>\n<li>将输入层向量和隐藏层向量初始状态值传给实例化后的对象，获得RNN的输出。</li>\n</ol>\n<p>在实例化该类时，需要传入如下属性：</p>\n<ul>\n<li>input_size：输入层神经元个数；</li>\n<li>hidden_size：每层隐藏层的神经元个数；</li>\n<li>num_layers：隐藏层层数，默认设置为1层；</li>\n<li>nonlinearity：激活函数的选择，可选是’tanh’或者’relu’，默认设置为’tanh’；</li>\n<li>bias：偏置系数，可选是’True’或者’False’，默认设置为’True’；</li>\n<li>batch_first：可选是’True’或者’False’，默认设置为’False’；</li>\n<li>dropout：默认设置为0。若为非0，将在除最后一层的每层RNN输出上引入Dropout层，dropout概率就是该非零值；</li>\n<li>bidirectional：默认设置为False。若为True，即为双向RNN。</li>\n</ul>\n<p>RNN的输入有两个，一个是input，一个是h<sub>0</sub>。input就是输入层向量，h<sub>0</sub>就是隐藏层初始状态值。<br>若没有采用批量输入，则输入层向量的形状为(L, H<sub>in</sub>)；<br>若采用批量输入，且batch_first为False，则输入层向量的形状为(L, N, H<sub>in</sub>)；<br>若采用批量输入，且batch_first为True，则输入层向量的形状为(N, L, H<sub>in</sub>)；<br>对于(N, L, H<sub>in</sub>)，在文本输入时，可以按顺序理解为(每次输入几句话，每句话有几个字，每个字由多少维的向量表示)。</p>\n<p>若没有采用批量输入，则隐藏层向量的形状为(D * num_layers, H<sub>out</sub>)；<br>若采用批量输入，则隐藏层向量的形状为(D * num_layers, N, H<sub>out</sub>)；<br>注意，batch_first的设置对隐藏层向量的形状不起作用。</p>\n<p>RNN的输出有两个，一个是output，一个是h<sub>n</sub>。output包含了每个时间步最后一层的隐藏层状态，h<sub>n</sub>包含了最后一个时间步每层的隐藏层状态。<br>若没有采用批量输入，则输出层向量的形状为(L, D * H<sub>out</sub>)；<br>若采用批量输入，且batch_first为False，则输出层向量的形状为(L, N, D * H<sub>out</sub>)；<br>若采用批量输入，且batch_first为True，则输出层向量的形状为(N, L, D * H<sub>out</sub>)。</p>\n<p>参数解释：</p>\n<ul>\n<li>N代表的是批量大小；</li>\n<li>L代表的是输入的序列长度；</li>\n<li>若是双向RNN，则D的值为2；若是单向RNN，则D的值为1；</li>\n<li>H<sub>in</sub>在数值上是输入层神经元个数；</li>\n<li>H<sub>out</sub>在数值上是隐藏层神经元个数。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\">rnn = nn.RNN(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">1</span>, batch_first=<span class=\"literal\">True</span>)  <span class=\"comment\"># 实例化一个单向单层RNN</span></span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">h0 = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">output, hn = rnn(<span class=\"built_in\">input</span>, h0)</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"手写复现\"><a href=\"#手写复现\" class=\"headerlink\" title=\"手写复现\"></a>手写复现</h1><h2 id=\"复现代码\"><a href=\"#复现代码\" class=\"headerlink\" title=\"复现代码\"></a>复现代码</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyRNN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.input_size = input_size</span><br><span class=\"line\">        self.hidden_size = hidden_size</span><br><span class=\"line\">        self.weight_ih = torch.randn(self.hidden_size, self.input_size) * <span class=\"number\">0.01</span></span><br><span class=\"line\">        self.weight_hh = torch.randn(self.hidden_size, self.hidden_size) * <span class=\"number\">0.01</span></span><br><span class=\"line\">        self.bias_ih = torch.randn(self.hidden_size)</span><br><span class=\"line\">        self.bias_hh = torch.randn(self.hidden_size)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span>, h0</span>):</span><br><span class=\"line\">        N, L, input_size = <span class=\"built_in\">input</span>.shape</span><br><span class=\"line\">        output = torch.zeros(N, L, self.hidden_size)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(L):</span><br><span class=\"line\">            x = <span class=\"built_in\">input</span>[:, t, :].unsqueeze(<span class=\"number\">2</span>)  <span class=\"comment\"># 获得当前时刻的输入特征，[N, input_size, 1]。unsqueeze(n)，在第n维上增加一维</span></span><br><span class=\"line\">            w_ih_batch = self.weight_ih.unsqueeze(<span class=\"number\">0</span>).tile(N, <span class=\"number\">1</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size, input_size]</span></span><br><span class=\"line\">            w_hh_batch = self.weight_hh.unsqueeze(<span class=\"number\">0</span>).tile(N, <span class=\"number\">1</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size, hidden_size]</span></span><br><span class=\"line\">            w_times_x = torch.bmm(w_ih_batch, x).squeeze(-<span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size]。squeeze(n)，在第n维上减小一维</span></span><br><span class=\"line\">            w_times_h = torch.bmm(w_hh_batch, h0.unsqueeze(<span class=\"number\">2</span>)).squeeze(-<span class=\"number\">1</span>)  <span class=\"comment\"># [N, hidden_size]</span></span><br><span class=\"line\">            h0 = torch.tanh(w_times_x + self.bias_ih  + w_times_h + self.bias_hh)</span><br><span class=\"line\">            output[:, t, :] = h0</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, h0.unsqueeze(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"验证正确性\"><a href=\"#验证正确性\" class=\"headerlink\" title=\"验证正确性\"></a>验证正确性</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">my_rnn = MyRNN(<span class=\"number\">10</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">h0 = torch.randn(<span class=\"number\">5</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">my_output, my_hn = my_rnn(<span class=\"built_in\">input</span>, h0)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape == my_output.shape, hn.shape == my_hn.shape)</span><br></pre></td></tr></table></figure>\n\n<pre><code>True True\n</code></pre>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\">官方说明文档</a></p>\n","categories":["NLP"]},{"title":"初学RNN","url":"/2022/11/15/21-46-57/","content":"<h1 id=\"FNN\"><a href=\"#FNN\" class=\"headerlink\" title=\"FNN\"></a>FNN</h1><h2 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>FNN（Feedforward Neural Network），即前馈神经网络，它是网络信息单向传递的一种神经网络，数据由输入层开始输入，依次流入隐藏层各层神经元，最终由输出层输出。其当前的输出只由当前的输入决定，任何层的输出都不会影响同级层。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/前馈神经网络-导出.png\" width=\"60%\"> \n</div> \n\n<p>以上图所示的神经网络为例，它在训练过程中通过前向计算和反向传播，不断通过调整权重系数W<sub>i</sub>和W<sub>o</sub>来实现学习目的。通常情况下，前馈神经网络会在空间上进行延伸，通过增加隐藏层层数与隐藏层神经元个数追求更好的学习效果。</p>\n<h2 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h2><p>前馈神经网络假定元素之间是相互独立的，对于序列数据，只能单独地处理序列中的每个元素，前一个输入与后一个输入在处理过程中也是完全独立的，无法捕获序列之间的依赖关系。</p>\n<h1 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h1><h2 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>RNN（Recurrent Neural Network），即循环神经网络，它是一种主要用来处理序列数据的神经网络，它关注了隐藏层每个神经元在时间维度上的变化，其中循环说成递推可能会更直观一些，本质就是同一个网络接收当前时刻的输入和上一时刻隐藏层神经元的输出，沿着时序反复迭代以实现对序列数据的学习。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><h3 id=\"直观结构\"><a href=\"#直观结构\" class=\"headerlink\" title=\"直观结构\"></a>直观结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/RNN-导出.png\" width=\"60%\"> \n</div> \n\n<p>在上图中， RNN 的每个时刻，输入层的x<sub>1</sub>和x<sub>2</sub>都在W<sub>i</sub>的作用下传入隐藏层，上一时刻的隐藏层输出也通过W<sub>h</sub>传入当前的隐藏层，因此它相当于可以间接访问之前的所有输入，这就是为什么说RNN可以保存记忆。</p>\n<h3 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/rnn.svg\" width=\"100%\"> \n</div> \n\n<p>上图展示了RNN在三个相邻时刻的计算逻辑。 在任意时刻t，隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>拼接t时刻的输入X<sub>t</sub>和t−1时刻的隐藏层状态H<sub>t−1</sub>，得到新的张量[X<sub>t</sub>,H<sub>t-1</sub>]；</li>\n<li>将新的张量送入带有激活函数φ的全连接层，激活函数常用tanh或者relu， 全连接层的输出是t时刻的隐藏层状态H<sub>t</sub>。</li>\n</ol>\n<p>在t时刻，隐藏层状态H<sub>t</sub>的计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/H公式.png\" width=\"100%\"> \n</div> \n\n<p>在t时刻，输出层的输出计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/O公式.png\" width=\"100%\"> \n</div> \n\n<p>参数说明：</p>\n<ul>\n<li>X<sub>t</sub>是t时刻的输入，它是一个向量；</li>\n<li>W<sub>i</sub>是输入层到隐藏层的权重矩阵；</li>\n<li>H<sub>t-1</sub>是t-1时刻的隐藏层状态，在初始时刻，会给隐藏层设置初始状态H<sub>0</sub>；</li>\n<li>W<sub>h</sub>是隐藏层上一时刻的值作用于当前时刻的权重矩阵；</li>\n<li>W<sub>o</sub>是隐藏层到输出层的权重矩阵；</li>\n<li>b<sub>h</sub>是和b<sub>o</sub>是偏置系数。</li>\n</ul>\n<p>注意：</p>\n<ul>\n<li>在不同时刻，RNN总是使用这些模型参数，其参数开销不会随着时间的增加而增加。</li>\n<li>隐藏层状态中X<sub>t</sub>W<sub>i</sub>+H<sub>t-1</sub>W<sub>h</sub>的计算，相当于X<sub>t</sub>和H<sub>t-1</sub>的拼接与W<sub>i</sub>和W<sub>h</sub>的拼接进行矩阵乘法。</li>\n<li>RNN和前馈神经网络一样，也是通过反向传播来更新权重，以达到学习的效果。</li>\n</ul>\n<h2 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h2><p>1-N类型，输入一张图片，输出一段话或者一段音乐，利用它可以实现看图说话。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/1-N.png\" width=\"50%\"> \n</div> \n\n<p>N-1类型，输入一段话，输出对其情感类别的判断，利用它可以实现文本分类。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-1.png\" width=\"50%\"> \n</div> \n\n<p>N-N类型，输入和输出是等长的序列，可以用来生成等长的诗歌。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-N.png\" width=\"50%\"> \n</div> \n\n<p>N-M类型，输入和输出是不等长的序列，也被叫做Encoder-Decoder模型或Seq2Seq模型，可以应用在机器翻译、文本摘要、阅读理解等多个领域上。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M.png\" width=\"50%\"> \n</div> \n\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M2.png\" width=\"90%\"> \n</div> \n\n<h2 id=\"变体\"><a href=\"#变体\" class=\"headerlink\" title=\"变体\"></a>变体</h2><h3 id=\"BRNN（Bidirectional-RNN）\"><a href=\"#BRNN（Bidirectional-RNN）\" class=\"headerlink\" title=\"BRNN（Bidirectional RNN）\"></a>BRNN（Bidirectional RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/BRNN.png\" width=\"100%\"> \n</div> \n\n<p>RNN的一个主要问题是只能从以往的输入进行学习，也就是只能理解上下文中的上文，为了拥有更好的学习效果，因此提出了双向RNN，也就是BRNN。 BRNN会在原有RNN的基础上再加一个隐藏层，该隐藏层的状态是从后向前传播的，从序列的终点开始读取，称为后向层；而原有的从序列起点开始读取的隐藏层称为前向层。 BRNN的隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>根据输入序列计算前向层隐藏层状态H<sub>1</sub>；</li>\n<li>将输入序列反转，计算后向层隐藏层状态H<sub>2</sub>；</li>\n<li>将H<sub>1</sub>和H<sub>2</sub>拼接起来，得到最终隐藏层状态H，H&#x3D;[H<sub>1</sub>,H<sub>2</sub>]。</li>\n</ol>\n<p>注意，只有能拿到整个输入序列时才能使用BRNN 。</p>\n<h3 id=\"DRNN（Deep-RNN）\"><a href=\"#DRNN（Deep-RNN）\" class=\"headerlink\" title=\"DRNN（Deep RNN）\"></a>DRNN（Deep RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/DRNN.png\" width=\"50%\"> \n</div> \n\n<p>与前馈神经网络不同，RNN因为考虑了时间维度，隐藏层达到三层就算多的了。</p>\n<h2 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a>优缺点</h2><h3 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h3><ul>\n<li>RNN可以处理序列信息，且内部结构简单，对计算资源的要求低。</li>\n</ul>\n<h3 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><ul>\n<li><p>RNN是一个时序模型，每个时刻的计算都依赖于前一时刻的结果，计算速度慢；</p>\n</li>\n<li><p>RNN由于梯度消失，难以支持长序列，不能捕获序列中长期的依赖关系；</p>\n</li>\n<li><p>RNN网络在时间维度上是串联的，离当前时间越远的隐藏层输出，对当前隐藏层的输出影响越小，它无法根据不同词本身的重要性对当前的输出产生影响；</p>\n</li>\n<li><p>RNN对所有输入是同等对待的，提取了所有的信息，没有区分有用信息、无用信息和辅助信息。但是如果某个网络可以根据不同输入的重要性，选择性地丢弃和记忆，就可以使得有效信息即使距离当前时间较远，也能有较大影响，实现长期记忆，这就引出了LSTM。</p>\n</li>\n</ul>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><ul>\n<li><a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-rnn-w-hidden-states\">《动手学深度学习》第8章第4节</a>；</li>\n<li>网络上相关资料。</li>\n</ul>\n","categories":["NLP"]}]