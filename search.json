[{"title":"初学RNN","url":"/2022/11/15/21-46-57/","content":"<h1 id=\"FNN\"><a href=\"#FNN\" class=\"headerlink\" title=\"FNN\"></a>FNN</h1><h2 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>FNN（Feedforward Neural Network），即前馈神经网络，它是网络信息单向传递的一种神经网络，数据由输入层开始输入，依次流入隐藏层各层神经元，最终由输出层输出。其当前的输出只由当前的输入决定，任何层的输出都不会影响同级层。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/前馈神经网络-导出.png\" width=\"60%\"> \n</div> \n\n<p>以上图所示的神经网络为例，它在训练过程中通过前向计算和反向传播，不断通过调整权重系数W<sub>i</sub>和W<sub>o</sub>来实现学习目的。通常情况下，前馈神经网络会在空间上进行延伸，通过增加隐藏层层数与隐藏层神经元个数追求更好的学习效果。</p>\n<h2 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h2><p>前馈神经网络假定元素之间是相互独立的，对于序列数据，只能单独地处理序列中的每个元素，前一个输入与后一个输入在处理过程中也是完全独立的，无法捕获序列之间的依赖关系。</p>\n<h1 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h1><h2 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>RNN（Recurrent Neural Network），即循环神经网络，它是一种主要用来处理序列数据的神经网络，它关注了隐藏层每个神经元在时间维度上的变化，其中循环说成递推可能会更直观一些，本质就是同一个网络接收当前时刻的输入和上一时刻隐藏层神经元的输出，沿着时序反复迭代以实现对序列数据的学习。</p>\n<h2 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h2><h3 id=\"直观结构\"><a href=\"#直观结构\" class=\"headerlink\" title=\"直观结构\"></a>直观结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/RNN-导出.png\" width=\"60%\"> \n</div> \n\n<p>在上图中， RNN 的每个时刻，输入层的x<sub>1</sub>和x<sub>2</sub>都在W<sub>i</sub>的作用下传入隐藏层，上一时刻的隐藏层输出也通过W<sub>h</sub>传入当前的隐藏层，因此它相当于可以间接访问之前的所有输入，这就是为什么说RNN可以保存记忆。</p>\n<h3 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/rnn.svg\" width=\"100%\"> \n</div> \n\n<p>上图展示了RNN在三个相邻时刻的计算逻辑。 在任意时刻t，隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>拼接t时刻的输入X<sub>t</sub>和t−1时刻的隐藏层状态H<sub>t−1</sub>，得到新的张量[X<sub>t</sub>,H<sub>t-1</sub>]；</li>\n<li>将新的张量送入带有激活函数φ的全连接层，激活函数常用tanh或者relu， 全连接层的输出是t时刻的隐藏层状态H<sub>t</sub>。</li>\n</ol>\n<p>在t时刻，隐藏层状态H<sub>t</sub>的计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/H公式.png\" width=\"100%\"> \n</div> \n\n<p>在t时刻，输出层的输出计算公式为：</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/O公式.png\" width=\"100%\"> \n</div> \n\n<p>参数说明：</p>\n<ul>\n<li>X<sub>t</sub>是t时刻的输入，它是一个向量；</li>\n<li>W<sub>i</sub>是输入层到隐藏层的权重矩阵；</li>\n<li>H<sub>t-1</sub>是t-1时刻的隐藏层状态，在初始时刻，会给隐藏层设置初始状态H<sub>0</sub>；</li>\n<li>W<sub>h</sub>是隐藏层上一时刻的值作用于当前时刻的权重矩阵；</li>\n<li>W<sub>o</sub>是隐藏层到输出层的权重矩阵；</li>\n<li>b<sub>h</sub>是和b<sub>o</sub>是偏置系数。</li>\n</ul>\n<p>注意：</p>\n<ul>\n<li>在不同时刻，RNN总是使用这些模型参数，其参数开销不会随着时间的增加而增加。</li>\n<li>隐藏层状态中X<sub>t</sub>W<sub>i</sub>+H<sub>t-1</sub>W<sub>h</sub>的计算，相当于X<sub>t</sub>和H<sub>t-1</sub>的拼接与W<sub>i</sub>和W<sub>h</sub>的拼接进行矩阵乘法。</li>\n<li>RNN和前馈神经网络一样，也是通过反向传播来更新权重，以达到学习的效果。</li>\n</ul>\n<h2 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h2><p>1-N类型，输入一张图片，输出一段话或者一段音乐，利用它可以实现看图说话。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/1-N.png\" width=\"50%\"> \n</div> \n\n<p>N-1类型，输入一段话，输出对其情感类别的判断，利用它可以实现文本分类。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-1.png\" width=\"50%\"> \n</div> \n\n<p>N-N类型，输入和输出是等长的序列，可以用来生成等长的诗歌。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-N.png\" width=\"50%\"> \n</div> \n\n<p>N-M类型，输入和输出是不等长的序列，也被叫做Encoder-Decoder模型或Seq2Seq模型，可以应用在机器翻译、文本摘要、阅读理解等多个领域上。</p>\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M.png\" width=\"50%\"> \n</div> \n\n<div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/N-M2.png\" width=\"90%\"> \n</div> \n\n<h2 id=\"变体\"><a href=\"#变体\" class=\"headerlink\" title=\"变体\"></a>变体</h2><h3 id=\"BRNN（Bidirectional-RNN）\"><a href=\"#BRNN（Bidirectional-RNN）\" class=\"headerlink\" title=\"BRNN（Bidirectional RNN）\"></a>BRNN（Bidirectional RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/BRNN.png\" width=\"100%\"> \n</div> \n\n<p>RNN的一个主要问题是只能从以往的输入进行学习，也就是只能理解上下文中的上文，为了拥有更好的学习效果，因此提出了双向RNN，也就是BRNN。 BRNN会在原有RNN的基础上再加一个隐藏层，该隐藏层的状态是从后向前传播的，从序列的终点开始读取，称为后向层；而原有的从序列起点开始读取的隐藏层称为前向层。 BRNN的隐藏层状态的计算可以被视为：</p>\n<ol>\n<li>根据输入序列计算前向层隐藏层状态H<sub>1</sub>；</li>\n<li>将输入序列反转，计算后向层隐藏层状态H<sub>2</sub>；</li>\n<li>将H<sub>1</sub>和H<sub>2</sub>拼接起来，得到最终隐藏层状态H，H&#x3D;[H<sub>1</sub>,H<sub>2</sub>]。</li>\n</ol>\n<p>注意，只有能拿到整个输入序列时才能使用BRNN 。</p>\n<h3 id=\"DRNN（Deep-RNN）\"><a href=\"#DRNN（Deep-RNN）\" class=\"headerlink\" title=\"DRNN（Deep RNN）\"></a>DRNN（Deep RNN）</h3><div align=\"center\"> \n<img src=\"/2022/11/15/21-46-57/DRNN.png\" width=\"50%\"> \n</div> \n\n<p>与前馈神经网络不同，RNN因为考虑了时间维度，隐藏层达到三层就算多的了。</p>\n<h2 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a>优缺点</h2><h3 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h3><ul>\n<li>RNN可以处理序列信息，且内部结构简单，对计算资源的要求低。</li>\n</ul>\n<h3 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><ul>\n<li><p>RNN是一个时序模型，每个时刻的计算都依赖于前一时刻的结果，计算速度慢；</p>\n</li>\n<li><p>RNN由于梯度消失，难以支持长序列，不能捕获序列中长期的依赖关系；</p>\n</li>\n<li><p>RNN网络在时间维度上是串联的，离当前时间越远的隐藏层输出，对当前隐藏层的输出影响越小，它无法根据不同词本身的重要性对当前的输出产生影响；</p>\n</li>\n<li><p>RNN对所有输入是同等对待的，提取了所有的信息，没有区分有用信息、无用信息和辅助信息。但是如果某个网络可以根据不同输入的重要性，选择性地丢弃和记忆，就可以使得有效信息即使距离当前时间较远，也能有较大影响，实现长期记忆，这就引出了LSTM。</p>\n</li>\n</ul>\n<h1 id=\"主要参考\"><a href=\"#主要参考\" class=\"headerlink\" title=\"主要参考\"></a>主要参考</h1><ul>\n<li><a href=\"https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-rnn-w-hidden-states\">《动手学深度学习》第8章第4节</a>；</li>\n<li>网络上相关资料。</li>\n</ul>\n","categories":["NLP"]}]